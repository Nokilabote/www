---
title: "Journaling, FFS and LFS"
date: 2016-04-11
author: Geoffrey Challen
description: >
  Discussion of journaling, the UNIX Fast File System (FFS) and log-structured
  file systems (LFS).
spelling_exceptions:
  - UFS
  - Kirk McKusick
  - John Ousterhout
  - Mendel Rosenblum
---
[.nooutline.spelling_exception]
== Technical Women

image::women/003.jpg[width="100%",title="Kristina Johnson",link="https://en.wikipedia.org/wiki/Kristina_M._Johnson"]

[.nooutline]
== Today

* Journaling
* The Berkeley Fast File System (FFS)
* Log-structured file systems (LFS)

[.nooutline]
== $ cat announce.txt

[.slider]
* Do ASST3!

== Another Approach to Consistency

[.slider]
* What's _not_ atomic? [.slide]#Writing *multiple* disk blocks.#
* What _is_ atomic? [.slide]#Writing *one* disk block.#

== Journaling

[.slider]
* Track pending changes to the file system in a special area on disk
called the *journal*.
* Following a failure, *replay* the journal to bring the file system back
to a consistent state.

[.slide.small]
--
Creation example:
____
Dear Journal, here's what I'm going to do today:

1.  Allocate inode 567 for a new file.
2.  Associate data blocks 5, 87, and 98 with inode 567.
3.  Add inode 567 to the directory with inode 33.
4.  That's it!
____
--

== Journaling: Checkpoints

[.slider]
.What happens when we flush cached data to disk?
* Update the journal!
* This is called a *checkpoint*.

[.slide.small]
____
Dear Journal, here's what I'm going to do today:

1.  [line-through]*Allocate inode 567 for a new file.*
2.  [line-through]*Associate data blocks 5, 87, and 98 with inode 567.*
3.  [line-through]*Add inode 567 to the directory with inode 33.*
4.  [line-through]*That's it!*

Dear Journal, I already did everything mentioned above! Checkpoint!
____

== Journaling: Recovery

[.slider]
.What happens on recovery?
* Start at the *last checkpoint* and work forward, updating on-disk
structures as needed.

[.slide.small]
____
Dear Journal, I already did everything mentioned above! Checkpoint!

Dear Journal, here's what I'm going to do today:

1. Allocate inode 567 for a new file. [.slide]*Did this already!*
2. Associate data blocks 5, 87, and 98 with inode 567. [.slide]*Didn't do
this... OK, done!*
3.  Add inode 567 to the directory with inode 33. [.slide]*Didn't do this
either! OK, done.*
4.  That's it! [.slide]*All caught up!*
____

== Journaling: Recovery

[.slider]
.What about incomplete journal entries?
* These are *ignored* as they may leave the file system in an incomplete
state.

[.slide.small]
--
What would happen if we processed the following incomplete journal entry?
____
Dear Journal, here's what I'm going to do today:

1.  Allocate inode 567 for a new file.
2.  Associate data blocks 5, 87, and 98 with inode 567.
____
--

== Journaling: Implications

Observation: *metadata* updates (allocate inode, free data block, add
to directory, etc.) can be represented compactly and probably written to
the journal *atomically*.

[.slider]
.What about data blocks themselves changed by write()?
* We could *include them in the journal* meaning that each data block
would potentially be written _twice_ (ugh).
* We could *exclude them from the journal* meaning that file system
structures are maintained but not file data.

== Caching, Consistency: Questions?

== The Berkeley Fast File System

[.slider]
* First included in the Berkeley Software Distribution (BSD) UNIX
release in *August, 1982.*
* Developed by Kirk McKusick. (*One guy!*)
* FFS is the basis of the Unix File System (UFS), which is still in use
and still developed by Kirk today.

== Exploiting Geometry

FFS made _many_ contributions to file system design. Some were lasting,
others less so.

[.slider]
* The less lasting features had to do with tailoring file system
performance to disk geometry.

[.slider]
.What are some geometry-related questions that file systems might try to address?
* Where to put *inodes*?
* Where to put *data blocks*, particularly where with respect to the
inodes that they are linked to?
* Where to put *related* files?
* What files are *likely* to be related?

[.small]
== Introducing Standard File System Features

FFS also responded to many problems with earlier file system implementations
and introduced many common features we have already learned about.

[.slider]
.Early file systems had a *small block size* of 512 B.
* FFS introduced larger *4K blocks*.

[.slider]
.Early file systems had no way to allocate *contiguous blocks* on disk.
* FFS introduced an ordered free block list allowing contiguous or
near-contiguous block allocation.

[.slider]
.Early file systems lacked many features.
* FFS added *symbolic links*, *file
locking*, unrestricted *file name lengths* and user *quotas*.

== What's Close On Disk?

[.slider]
.Two enemies of closeness:
. Lateral movement, or seek times. This is *major*.
. Rotational movement or delay. This is *minor*.

image::figures/disks/chs.svg[width="80%"]

== Seek Planning: Cylinder Groups

*Cylinder group:* All of the data that can be read on the disk without
moving the head. Comes from *multiple platters*.

[.slider]
.On FFS each *cylinder group* has:
* A (backup) copy of the superblock.
* A cylinder-specific header with superblock-like statistics.
* A number of inodes.
* Data blocks.
* It's almost like it's own mini file system!

[.nooutline]
== It's Own Mini File System!

image::figures/disks/minime.jpg[width="40%"]

== Groups!

image::figures/disks/debugfs-show_super_stats2.svg[width="90%"]

== Rotational Planning

FFS superblock contained detailed disk geometry information allowing FFS to
attempt to perform better block placement.

[.slider]
.Example:
* Imagine that the speed at which the heads can read information off
the disk is greater than the speed at which the disk can return data to
the operating system.
* So the disk *cannot* read consecutive blocks off of the disk. Can't
read 0, 1, 2, 3, etc., because after I finish transferring Block 0 the
heads are over a different block.
* FFS will incorporate this delay and attempt to lay out consecutive
blocks for a single file as 0, 3, 6, 9, etc.
* *Wow/[.spelling_exception]#eww#!*

== Back to the Future

[.slider]
* Does this stuff matter anymore?
* If not, why not, and is it a good thing that it doesn't?

== Hardware Weenies v. Software Weenies

Continuing battle for the soul of your computer between the forces of light
(us/them) and darkness (them/us).

[.slider]
.Who should be responsible for making the slow disk fast?
* *Hardware:* *fixed* and *fast*.
* *Software:* *flexible* and *slow*.

== Put the OS in Control

[.small]
____
*OS*—Put this block exactly where I tell you to right now, slave!

*Disk*—Yes master.
____

[.slider]
.*Pros:*
* OS has better *visibility* into workloads, users, relationships,
consistency requirements, etc. This information can improve performance.

[.slider]
.*Cons:*
* Operating systems are slow and buggy.

== Leave it to Hardware

[.slide]
____
*OS*--Oh disk, you are so clever. Here's some data! I trust you'll do the
right thing with it.

*Disk*—I *am* so clever.
____

[.slider]
.*Pros:*
* The device knows much more about itself than the operating system can
be expected to.
* Device buffers and caches are closer to the disk.

[.slider]
.*Cons:*
* Device opaqueness may violate guarantees that the operating system is
trying to provide.

== FFS Continues To Improve

[.slider]
.*Block sizing:* continues to respond to changes to average file sizes.
* Small blocks: less internal fragmentation, more seeks.
* Large blocks: more internal fragmentation, fewer seeks.

[.slider]
.Co-locating inodes and directories.
* *Problem:* accessing directory contents is slow.
* *Solution:* jam inodes for directory _into_ directory file itself.

[.slide]
--
Separate solution to consistency called *soft updates*, which has
recently been combined with journaling. (Worth its own lecture.)
--

== FFS: Questions?

== On To The '90s

*FFS* circa 1982.

[.slider]
.Fast forward: it's now 1991. What's different?
* _"(Everything I Do) I Do It for You"_ replaces _"Eye of the Tiger"._
* _"The Silence of the Lambs"_ replaces _"Gandhi"_.

[.slide]
--
OK, so what's different about *disks*?
--

== Computers Circa 1991

[.slider]
* Disk *bandwidth* is improving rapidly, meaning the operating system
can stream reads or writes to the disk _faster_.
* Computers have *more memory*. Up to _128 MB_! (Whoa.)
* And, alas, disk seek times are ... [.slide]#still dog slow!#

== Using What We Got

So, if we can solve this pesky *seek* issue we can utilize growing disk
*bandwidth* to improve file system *performance*.

[.slider]
* Oh, by the way: we've got a bunch of spare memory lying around. Might
that be useful?

== Use a Cache!

[.slider]
.How do we make a big slow thing look faster?
* *Use a cache!* Or, put a smaller, faster thing in front of it.

[.slider]
* In the case of the file system the smaller, faster thing is [.slide]*memory.*
* We call the memory used to cache file system data the [.slide]*buffer cache.*

== A New Hope

[.slider]
* With a large cache, we should be able to avoid doing almost any disk
*reads*.
* But we will still have to do disk *writes*, but the cache will still
help collect small writes in memory until we can do one larger write..

[.slide]
--
Forget for a minute everything you've learned about file system design and
answer the following question:

[.slider]
.What's the best way to avoid *seeks* when *writing*?
* Write everything *to the same place*!
--

== Log Structured File Systems

Invented and implemented at Stanford by then-faculty John Ousterhout
and now-faculty Mendel Rosenblum.

[.slider]
.*Main idea:* all writes go to an *append-only* log.
* Great... um... how do we _do_ that?

== A Normal Write

Example: let's say we want to change an existing byte in a file.

[.slider]
.What would we normally do?
. *Seek* to _read_ the inode map.
. *Seek* to _read_ the inode.
. *Seek* to _write_ (modify) the data block.
. *Seek* to _write_ (update) the inode.

== A Normal Write

[.slide.replace]
--
image:figures/disks/normalseeks-1.svg[width="100%"]
--

[.slide.replace]
--
image:figures/disks/normalseeks-2.svg[width="100%"]
--

[.slide.replace]
--
image:figures/disks/normalseeks-3.svg[width="100%"]
--

[.slide.replace]
--
image:figures/disks/normalseeks-4.svg[width="100%"]
--

== A Cached-Read Write

Let's assume that our big friendly cache is going to soak up the reads.

[.slider]
.Now what happens?
. [line-through]**Seek* to _read_ the inode map.*
. [line-through]**Seek* to _read_ the inode.*
. *Seek* to _write_ (modify) the data block.
. *Seek* to _write_ (update) the inode.

== A Cached-Read Write

[.slide.replace]
--
image:figures/disks/cachedseeks-1.svg[width="100%"]
--

[.slide.replace]
--
image:figures/disks/cachedseeks-2.svg[width="100%"]
--

== An LFS Write

[.slide.replace]
--
image:figures/disks/lfsseeks-1.svg[width="100%"]
--

[.slide.replace]
--
image:figures/disks/lfsseeks-2.svg[width="100%"]
--

[.slide.replace]
--
image:figures/disks/lfsseeks-3.svg[width="100%"]
--

[.slide.replace]
--
image:figures/disks/lfsseeks-4.svg[width="100%"]
--

[.slide.replace]
--
image:figures/disks/lfsseeks-5.svg[width="100%"]
--

[.slide.replace]
--
image:figures/disks/lfsseeks-6.svg[width="100%"]
--

== I See What You Did There

Elegant solution! Reads are handled by the cache. Writes can stream to
the disk at full bandwidth due to short seeks to append to the log.

[.slider]
* But some *thorny details* to address.

== When Do Writes Happen?

Want to stream _as many writes_ to disk together as possible to
maximize usage of disk bandwidth.

[.slider]
.*When* do we write to the log?
* When the user calls `sync`, `fsync` or when blocks are evicted from the
buffer cache.

== Locating LFS inodes

[.slider]
.How did FFS translate an inode number to a disk block?
* It stored the inode map in a *fixed* location on disk.

[.slider]
.Why is this a problem for LFS?
* inodes are just appended to the log and so they can *move*!

[.slider]
.And so what do you think that LFS does about this?
* Yep, it *logs* the inode map.

== LFS metadata

[.slider]
.What about file system metadata: inode and data block allocation bitmaps, etc.?
* We can log that stuff too!

== As the Log Turns

[.slider]
.What happens when the log reaches the end of the disk?
* Probably a *lot* of unused space earlier in the log due to
overwritten inodes, data blocks, etc.

[.slider]
.How do we reclaim this space?
* *Clean* the log by identifying empty space and compacting used
blocks.

[.slide]
--
Conceptually you can think of this happening across the entire disk
all at once, but for performance reasons LFS divides the disk into
*segments* which are cleaned separately.
--

== LFS Cleaning

[.slide.replace]
--
image:figures/disks/logcleaning-1.svg[width="100%"]
--

[.slide.replace]
--
image:figures/disks/logcleaning-2.svg[width="100%"]
--

[.slide.replace]
--
image:figures/disks/logcleaning-3.svg[width="100%"]
--

[.slide.replace]
--
image:figures/disks/logcleaning-4.svg[width="100%"]
--

== The Devil's in the Cleaning

[.slider]
* LFS seems like an incredibly great idea... [.slide]#until you start thinking
about *cleaning*.#
* (Then it becomes a *debatably* great idea...)

== Cleaning Questions

[.slider]
.*When* should we run the cleaner?
* Probably when the system is _idle_, which may be a problem on systems
that don't idle much.

[.slider]
.What *size* segments should we clean?
* *Large* segments amortize the cost to read and write all of the data
necessary to clean the segment.
* ...but *small* segments increase the probability that _all_ blocks in
a segment will be "dead", making cleaning trivial.

[.slider]
.What other effect does log cleaning have?
* Cleaner overhead is *very* workload-dependent, making it difficult to
reason about the performance of log-structure file system. (And easy to
fight about their performance!)

== Reading Questions

Let's say that the cache *does not* soak up as many reads as we were
hoping.

[.slider]
.What problem can LFS create?
* Block allocation is extremely discontiguous, meaning that reads may
seek all over the disk.

[.smaller]
== People Care About This Stuff

[.slide]
--
* 1991: original LFS paper by Ousterhout and Rosenblum.
--
[.slide]
--
* 1993: reimplementation by Margo Seltzer questions performance
improvements:
____
"Unfortunately, an enhanced version of FFS (with read and write
clustering) provides comparable and sometimes superior performance to
our LFS."
____
--
[.slide]
--
* Ousterhout responds to the '93 critique and complains of "poor
BSD-LFS implementation", "poor benchmark choice", and "poor analysis."
--
[.slide]
--
* 1995: a second paper by Margo Seltzer _again_ questions LFS
performance claims.
____
For small files, both systems provide comparable read performance, but
LFS offers superior performance on writes. For large files (one megabyte
and larger), the performance of the two file systems is comparable. When
FFS is tuned for writing, its large-file write performance is
approximately 15% better than LFS, but its read performance is 25%
worse. When FFS is optimized for reading, its large-file read and write
performance is comparable to LFS.
____
--
[.slide]
--
* Ousterhout describes the '95 analysis as improved but states that
"the new paper is still misleading in several ways".
--

[.nooutline]
== What Else Did Margo Do?

[.slider]
* Initiated the creation of OS/161.
* Taught me operating systems.

[.nooutline]
== Next Time

* RAID: please read the paper!
